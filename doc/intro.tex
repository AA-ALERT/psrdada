
\chapter{Introduction}

\section{Design Goals}

\begin{enumerate}
\item modularity
\vspace{-2mm}
\item separation of data transport, control, and data reduction
\vspace{-2mm}
\item near real-time data reduction
\vspace{-2mm}
\item elegant scaling to slower data reduction speeds
\end{enumerate}

\section{Overview}

\subsection{Workstation Cluster}

The functionality of PuMa-II will be divided across multiple workstations.
These are divided into two main classes:

\begin{enumerate}

\item Primary Units - workstations equipped with Direct Memory Access 
	(DMA) card, large data storage, and high-speed interconnect.

\item Secondary Units - workstations equipped with high-speed interconnect
	and modest data storage facilities.

\end{enumerate}

A single Primary unit may have multiple Secondary units associated
with it.  To these, it sends the digitized data in round-robin
fashion.  The various components of the workstation cluster are
depicted in simplified form in Figure~\ref{fig:layout}.

\begin{figure}
\centerline{\psfig{figure=layout.eps,width=4in,angle=0}}
\caption [\sffamily PuMa-II Schematic Overview]
{
Schematic Overview of PuMa-II.  The fat line indicates the direction
of data flow.  Note the parallel use of the RAM Data Block on both
Primary and Secondary Units.
}
\label{fig:layout}
\end{figure}

\subsection{Software}

The major functionality of PuMa-II is divided into five categories:

\begin{enumerate}

\item Data Flow Control - the high-bandwidth data transfer control software
\item Data Reduction - process and archive the baseband data
\item Command and Monitoring - communication channels for external control
\item Control Interface - centralized access to Command and Monitoring
\item Configuration and Scheduling - files and databases for automated control

\end{enumerate}

Communication between each of the levels of software will take place
primarily through shared memory resources and internet socket
connections.

\subsubsection{Data Flow Control}

Data Flow Control software will establish the high-bandwidth network
connections between the Primary and Secondary units.  This software
will control all aspects pertaining to the flow of data from the DMA
card on the Primary unit to local storage and/or the remote RAM and/or
disk storage of the Secondary units.  In the case of online pulsar
processing, this communication will include the overlap required to
compensate for data reduction losses (owing to dispersion smearing,
filter rise times, etc.).  

On both Primary and Secondary units, data will be stored in a large
buffer established as shared memory, called the Data Block.  The
various tasks that must run in parallel will be implemented as unique
processes, as opposed to multiple threads of a single process.
Therefore, access to the Data Block will be controlled by an
inter-process locking method, such as a semaphore.

Rational: It is better to begin with multiple processes and
inter-process control in the early states of development because this
paradigm is more modular.  For example, the process that reads data
from the Data Block and writes it to local storage may be run on
either a primary or secondary node.  If data reduction can later be
performed in real-time, the disk writing client may be replaced by a
data processing client.

\subsubsection{Data Reduction}

On both Primary and Secondary units, one or more Data Clients may
attach to the Data Block and operate on the data.  The Data Block will
be logically divided into a number of sub-blocks.  Each sub-block may
be flagged as ``processed'' by the clients in order that the Data Flow
Control software may over-write the information contained by that
sub-block.  After writing each sub-block, the Data Flow Control
software will set a flag to indicate that the block contains new data.

A single, high-priority Data Client will be given permission to flag
sub-blocks as completed.  Initially, this client will be part of the
Data Flow Control software that writes the data to local file storage.
Later, this client may be a data processing client.  Data Clients may
perform any number of tasks, including various forms of data
reduction, calculation and display of data quality statistics such as
the bandpass, storage of the data to some form of medium, or farming
the data out to a grid.

The data reduction client will basically do as it is told, as
described in Configuration and Scheduling.

\subsubsection{Command and Monitoring}

Command and Monitoring software includes the Command software that
establishes low-bandwidth network communication channels between
Primary and Secondary units and the Control Interfaces.  These
channels are used for sending high-level control commands, such as
start, stop, and information about the source.  These communication
channels may be implemented as a control thread in each component of
the Data Flow Control software. 

The Monitoring software will perform any tasks required to maintain
proper operation of the instrument and present useful information to
the user.  This includes monitoring telescope status information from
TCS, disk space consumption, network traffic, CPU load, etc.

\subsubsection{Control Interface}

The Control Interface software defines the centralized command/control
point, and will be connected to the various communication channels
established by the Control and Monitoring software on each of the
Primary and Secondary units.  The Control Interface may be run on any
workstation, and provides the means through which other processes may
treat PuMa-II as a single instrument.  For example, a text or
graphical user interface and/or automated scheduling program may
connect to PuMa-II, issue commands, and inquire about the status of
the instrument.  A textual user interface (TUI) will be developed to
connect to the Control Interface and:
\begin{itemize}
\item allow PuMa-II to be configured, started, and stopped;
\vspace{-2mm}
\item display various status variables; and
\vspace{-2mm}
\item create plots of diagnostic graphs, 
      such as the passband and digitizer statistics
\end{itemize}

\subsubsection{Configuration and Scheduling}

Configuration and scheduling will be make use of a database interface.
TO DO: Describe this in more detail.
